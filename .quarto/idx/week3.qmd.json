{"title":"Week 3: Data Corrections","markdown":{"yaml":{"title":"Week 3: Data Corrections"},"headingText":"Summary","containsRefs":false,"markdown":"\n\n\nThis week we looked at possible data errors that can arise during the data collection process and how they can be corrected. These largely concern either geometric or atmospheric errors. We also examined methods of visual enhancement of certain data characteristics that allow us to learn more about specific aspects of the environment difficult to otherwise pick out from regular colorband representations.\n\n### Geometric error calibration\n\nGeometric errors often arise from miscalibration in where the camera is pointed, leading to data being off-kilter from their true locations. The view angle may be off nadir, or the satellite may be slightly off its orbit path. This issue can be fixed with by identifying control points within the incorrect data and a control image, identifying their spatial relationship and applying a linear regression to convert all of the data to the coordinates of the control data.\n\n### Atmospheric error calibration\n\nAtmospheric errors arise from light scattering against atmospheric particles and entering the lens, appearing as haze. Or in more scientific terms, additive path radiance can cause at sensor radiance to differ from surface total radiance. This can be corrected through absolute or relative methods. Absolute methods use atmospheric data or satellite targets to measure the amount of atmospheric interference and automatically correct it. Relative methods approximate this adjustment without access to other forms of data, through use of methods such as dark object subtraction or pseudo-invariant features. These methods estimate what the data should look like based on a couple of reference points and transforms the data to fit those references.\n\n### Merging\n\nIn today's activity we examined how to merge multiple observation tiles to create a larger image, useful when studying a large area. Other forms of merging can combine different kinds of sensor outputs, such as the merging of active and passive sensors, to enhance data from a study area. Other forms of data enhancement are also possible without merging; NDVI indexes can be visually represented to show vegetation levels and health more clearly, and texture can also be represented to show the relationships between the values of different pixels more clearly.\n\n## Applications\n\nTwo interesting applications of merging can be found in Rudner et al (2019) and Belenguer-Plomer et al (2021). Both of these studies merge optical band data from Sentinel-2 with radar data from Sentinel-1 to gain more information in the wake of natural disasters that can obscure typical remote sensing methods.\n\nIn Rudner et al, researchers aimed to remediate the issues of limited or slow satellite data in the wake of floods, when data is urgently needed for search and rescue operations in order to prioritise the areas worst affected. They combined data of not only multiple types (optical and radar) but also multiple temporalities (before and after the disaster) and multiple resolutions. These methods focus on speed of data availability to first responders. Radar data is used to immediately identify which areas have been flooded as water responds differently to radar than buildings do, and can be detected even in stormy weather that can prevent optical band sensing. Optical bands can come in handy later once water levels have fallen and \"before\" images can be compared to \"after\" images to ascertain areas of worst damage. ![Layers of data inputs and trained outputs predicting flooded building footprints from \"Multi3net: segmenting flooded buildings via fusion of multiresolution, multisensor, and multitemporal satellite imagery\"](images/3.2.png)\n\nIn Belenguer-Plomer et al, ten different environment types around the world were recorded with optical band data and radar data before and after fires, in order to develop a neural network capable of identifying burned areas and estimating the severity of the burn. The following data is fed into a convolutional neural network; the Normalised Difference Vegetation Index (NDVI), the Normalised Burn Ratio, the Normalised Difference Water Index, the Mid Infrared Burn Index, and the optical band data from before and after the fire. These indices are all obtained from data enhancement methods studied this week. The use of radar in this case also contributed data on structural change and moisture that could give clues to which areas had and had not been burned. After training on one set of sites, the neural network was tested on another set of sites with similar biomes, showing varying accuracy levels across different types of land cover. They recorded increased accuracy when adding radar data to the usual optical band information used.\n\n![CNN accuracy rates at predicting if different land cover types have been burned with Sentinel 1 SAR, Sentinel 2 colour bands, and a combination of both from \"CNN-based burned area mapping using radar and optical data\"](images/3.1.jpg)\n\nBoth of these studies engage in complex pre-processing and data integration methods, and both also rely on convolutional neural networks (CNN's) for processing the integrated data and identifying areas of concern. Both succeed in certain areas while possibly falling short in others.\n\nRudner et al can deliver results quickly, as it uses data captured before, during, and after the floods with the intent of making the data readable and actionable at all phases. However, due to the narrow breadth of its training scenario, it could struggle under alternative conditions such as different flood patterns or another type of landscape. Belenguer-Plomer et al shows the necessity of adjusting algorithms to be prepared for different scenarios and environments, as the neural network was shown to have varying accuracy rates in differing biomes even when taking that variation into account during the training process. Belenguer-Plomer et al also reports its accuracy levels, which Rudner et al fails to do. However, Belenguer-Plomer et al also has certain shortcomings. Due to the extensive preprocessing required and the need for data captured before and after the event, it likely would not have the same rapid applicability as the Rudner et al study.\n\n## Reflections\n\nSome of the \"radiance\" vs \"reflectance\" etc jargon is difficult to remember. I found the use of linear regression for image correction to be very interesting as I use linear regression often in data science research contexts but have not thought much about its practical uses in geography. Even though the process sounds complicated and I don't know if I want to get too far into it, particularly because corrected data is already easy to access, this has made me more interested in the transformative applications of regression equations. It's interesting to hear about how different issues can arise and the correction methods implemented to solve them, and I understand why this is important for us to know, but I still find myself more interested in the enhancement aspects and particularly in the NDVI index. In my undergraduate degree I studied environmental anthropology and invasive ecosystems, so it is super cool to see remote sensing used in research on the topic.\n\n## References\n\nBelenguer-Plomer, M.A., Tanase, M.A., Chuvieco, E. and Bovolo, F., 2021. \"CNN-based burned area mapping using radar and optical data\". Remote Sensing of Environment, 260, p.112468. https://www.sciencedirect.com/science/article/pii/S0034425721001863?via%3Dihub\n\n​​Rudner, T.G., Rußwurm, M., Fil, J., Pelich, R., Bischke, B., Kopačková, V. and Biliński, P., 2019, July. \"Multi3net: segmenting flooded buildings via fusion of multiresolution, multisensor, and multitemporal satellite imagery\". In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 33, No. 01, pp. 702-709). https://ojs.aaai.org/index.php/AAAI/article/view/3848\n","srcMarkdownNoYaml":"\n\n## Summary\n\nThis week we looked at possible data errors that can arise during the data collection process and how they can be corrected. These largely concern either geometric or atmospheric errors. We also examined methods of visual enhancement of certain data characteristics that allow us to learn more about specific aspects of the environment difficult to otherwise pick out from regular colorband representations.\n\n### Geometric error calibration\n\nGeometric errors often arise from miscalibration in where the camera is pointed, leading to data being off-kilter from their true locations. The view angle may be off nadir, or the satellite may be slightly off its orbit path. This issue can be fixed with by identifying control points within the incorrect data and a control image, identifying their spatial relationship and applying a linear regression to convert all of the data to the coordinates of the control data.\n\n### Atmospheric error calibration\n\nAtmospheric errors arise from light scattering against atmospheric particles and entering the lens, appearing as haze. Or in more scientific terms, additive path radiance can cause at sensor radiance to differ from surface total radiance. This can be corrected through absolute or relative methods. Absolute methods use atmospheric data or satellite targets to measure the amount of atmospheric interference and automatically correct it. Relative methods approximate this adjustment without access to other forms of data, through use of methods such as dark object subtraction or pseudo-invariant features. These methods estimate what the data should look like based on a couple of reference points and transforms the data to fit those references.\n\n### Merging\n\nIn today's activity we examined how to merge multiple observation tiles to create a larger image, useful when studying a large area. Other forms of merging can combine different kinds of sensor outputs, such as the merging of active and passive sensors, to enhance data from a study area. Other forms of data enhancement are also possible without merging; NDVI indexes can be visually represented to show vegetation levels and health more clearly, and texture can also be represented to show the relationships between the values of different pixels more clearly.\n\n## Applications\n\nTwo interesting applications of merging can be found in Rudner et al (2019) and Belenguer-Plomer et al (2021). Both of these studies merge optical band data from Sentinel-2 with radar data from Sentinel-1 to gain more information in the wake of natural disasters that can obscure typical remote sensing methods.\n\nIn Rudner et al, researchers aimed to remediate the issues of limited or slow satellite data in the wake of floods, when data is urgently needed for search and rescue operations in order to prioritise the areas worst affected. They combined data of not only multiple types (optical and radar) but also multiple temporalities (before and after the disaster) and multiple resolutions. These methods focus on speed of data availability to first responders. Radar data is used to immediately identify which areas have been flooded as water responds differently to radar than buildings do, and can be detected even in stormy weather that can prevent optical band sensing. Optical bands can come in handy later once water levels have fallen and \"before\" images can be compared to \"after\" images to ascertain areas of worst damage. ![Layers of data inputs and trained outputs predicting flooded building footprints from \"Multi3net: segmenting flooded buildings via fusion of multiresolution, multisensor, and multitemporal satellite imagery\"](images/3.2.png)\n\nIn Belenguer-Plomer et al, ten different environment types around the world were recorded with optical band data and radar data before and after fires, in order to develop a neural network capable of identifying burned areas and estimating the severity of the burn. The following data is fed into a convolutional neural network; the Normalised Difference Vegetation Index (NDVI), the Normalised Burn Ratio, the Normalised Difference Water Index, the Mid Infrared Burn Index, and the optical band data from before and after the fire. These indices are all obtained from data enhancement methods studied this week. The use of radar in this case also contributed data on structural change and moisture that could give clues to which areas had and had not been burned. After training on one set of sites, the neural network was tested on another set of sites with similar biomes, showing varying accuracy levels across different types of land cover. They recorded increased accuracy when adding radar data to the usual optical band information used.\n\n![CNN accuracy rates at predicting if different land cover types have been burned with Sentinel 1 SAR, Sentinel 2 colour bands, and a combination of both from \"CNN-based burned area mapping using radar and optical data\"](images/3.1.jpg)\n\nBoth of these studies engage in complex pre-processing and data integration methods, and both also rely on convolutional neural networks (CNN's) for processing the integrated data and identifying areas of concern. Both succeed in certain areas while possibly falling short in others.\n\nRudner et al can deliver results quickly, as it uses data captured before, during, and after the floods with the intent of making the data readable and actionable at all phases. However, due to the narrow breadth of its training scenario, it could struggle under alternative conditions such as different flood patterns or another type of landscape. Belenguer-Plomer et al shows the necessity of adjusting algorithms to be prepared for different scenarios and environments, as the neural network was shown to have varying accuracy rates in differing biomes even when taking that variation into account during the training process. Belenguer-Plomer et al also reports its accuracy levels, which Rudner et al fails to do. However, Belenguer-Plomer et al also has certain shortcomings. Due to the extensive preprocessing required and the need for data captured before and after the event, it likely would not have the same rapid applicability as the Rudner et al study.\n\n## Reflections\n\nSome of the \"radiance\" vs \"reflectance\" etc jargon is difficult to remember. I found the use of linear regression for image correction to be very interesting as I use linear regression often in data science research contexts but have not thought much about its practical uses in geography. Even though the process sounds complicated and I don't know if I want to get too far into it, particularly because corrected data is already easy to access, this has made me more interested in the transformative applications of regression equations. It's interesting to hear about how different issues can arise and the correction methods implemented to solve them, and I understand why this is important for us to know, but I still find myself more interested in the enhancement aspects and particularly in the NDVI index. In my undergraduate degree I studied environmental anthropology and invasive ecosystems, so it is super cool to see remote sensing used in research on the topic.\n\n## References\n\nBelenguer-Plomer, M.A., Tanase, M.A., Chuvieco, E. and Bovolo, F., 2021. \"CNN-based burned area mapping using radar and optical data\". Remote Sensing of Environment, 260, p.112468. https://www.sciencedirect.com/science/article/pii/S0034425721001863?via%3Dihub\n\n​​Rudner, T.G., Rußwurm, M., Fil, J., Pelich, R., Bischke, B., Kopačková, V. and Biliński, P., 2019, July. \"Multi3net: segmenting flooded buildings via fusion of multiresolution, multisensor, and multitemporal satellite imagery\". In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 33, No. 01, pp. 702-709). https://ojs.aaai.org/index.php/AAAI/article/view/3848\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","output-file":"week3.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.545","bibliography":["references.bib"],"editor":"visual","theme":"cosmo","title":"Week 3: Data Corrections"},"extensions":{"book":{"multiFile":true}}},"pdf":{"identifier":{"display-name":"PDF","target-format":"pdf","base-format":"pdf"},"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"xelatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","output-file":"week3.pdf"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"block-headings":true,"bibliography":["references.bib"],"editor":"visual","documentclass":"scrreprt","title":"Week 3: Data Corrections"},"extensions":{"book":{"selfContainedOutput":true}}}},"projectFormats":["html","pdf"]}